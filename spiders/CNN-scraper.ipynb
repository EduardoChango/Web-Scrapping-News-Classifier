{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from scrapy.selector import Selector\n",
    "import time\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    " \n",
    "\n",
    "search_str = 'cybersecurity financial industry'\n",
    "last_x_weeeks = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Search terms defined abvove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "allowed_domains = ['www.cnn.com']\n",
    "start_urls = ['https://www.cnn.com']\n",
    "\n",
    "# set up the driver\n",
    "chrome_options = Options()\n",
    "# chrome_options.add_argument(\"--headless\") # uncomment if don't want to appreciate the sight of a possessed browser\n",
    "driver = webdriver.Chrome(chrome_options)\n",
    "driver.get(\"https://edition.cnn.com/search\")\n",
    "\n",
    "\n",
    "\n",
    "search_input = driver.find_element(by = By.XPATH,value=\"//*[@id='search']/div[1]/div[1]/input\") # find the search bar\n",
    "search_input.send_keys(search_str) # type in the search term\n",
    "\n",
    "time.sleep(1)\n",
    "search_btn = driver.find_element(by = By.XPATH,value = \"//*[@id='search']/div[1]/div[1]/button[2]\") # find the search button\n",
    "search_btn.click()\n",
    "\n",
    "time.sleep(1)\n",
    "stories_label = driver.find_element(by = By.XPATH,value = \"//*[@id='search']/div[1]/div[2]/div/div/ul/li[2]/label\") # find the search button\n",
    "stories_label.click()\n",
    "\n",
    "time.sleep(1)\n",
    "search_btn = driver.find_element(by = By.XPATH,value = \"//*[@id='relevance']\") # find the search button\n",
    "search_btn.click()\n",
    "\n",
    "time.sleep(2)\n",
    "driver.refresh()\n",
    "time.sleep(2)\n",
    "\n",
    "html = [driver.page_source]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Define function to scrap each news page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pages(html):\n",
    "    rows = []\n",
    "    for page in html:\n",
    "        resp = Selector(text = page)\n",
    "        #print(resp)\n",
    "        results = resp.css(\"#search > div.search__right > div > div.search__results-list > div > div.container_list-images-with-description__cards-wrapper > div > div\") # result iterator\n",
    "        #print(results)    \n",
    "        dataUriList =results.xpath(\"//*[@id='search']/div[2]/div/div[2]/div/div[2]/div/div\")\n",
    "        news_url = dataUriList.xpath(\"//*[@class='container__headline-text']\")\n",
    "        #print(news_url)\n",
    "        row = []\n",
    "        for i,item in enumerate(news_url):\n",
    "            \n",
    "            \n",
    "            title = item.xpath(\".//text()\").get()\n",
    "            #print(title)\n",
    "\n",
    "            \n",
    "            date_div = item.xpath(f\"//*[@id='search']/div[2]/div/div[2]/div/div[2]/div/div/div[{i+1}]/a[2]/div/div[2]\")\n",
    "            date = date_div.xpath(\".//text()\").get().strip()\n",
    "            datetime_parsed =datetime.strptime(date, \"%b %d, %Y\")\n",
    "            #print(date)\n",
    "\n",
    "\n",
    "            link = item.xpath(\".//@data-zjs-href\").get()\n",
    "            #print(link)\n",
    "\n",
    "\n",
    "            description_div = item.xpath(f\"//*[@id='search']/div[2]/div/div[2]/div/div[2]/div/div/div[{i+1}]/a[2]/div/div[3]\")\n",
    "            description = description_div.xpath(\".//text()\").get().strip()\n",
    "            #print(description)\n",
    "            author = 'CNN'\n",
    "            row = [title,datetime_parsed,description,link,author]\n",
    "            rows.append(row)\n",
    "\n",
    "        return rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Poll each page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = ['Title','Publish Date','Description','Url','Author']\n",
    "news = get_pages(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    \n",
    "    i+=1\n",
    "    time.sleep(2)\n",
    "    next_btn = driver.find_element(by = By.XPATH,value = \"//*[@id='search']/div[2]/div/div[4]/div/div[3]\")\n",
    "    next_btn.click()\n",
    "    time.sleep(2)\n",
    "    driver.refresh()\n",
    "    time.sleep(2)\n",
    "    news.extend(get_pages([driver.page_source]))\n",
    "\n",
    "print(len(news))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Order and filter news by date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_date = datetime.today()-timedelta(weeks=last_x_weeeks)\n",
    "\n",
    "\n",
    "news_df = pd.DataFrame(data=news,columns=headers)\n",
    "news_df_sorted = news_df.sort_values(by= 'Publish Date', ascending=False)\n",
    "news_df_filtered = news_df_sorted[news_df_sorted['Publish Date']>=filter_date]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Build the output file to be used as chat gpt input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Title Publish Date  \\\n",
      "37  Cyberattack disrupts operations at major US he...   2024-05-08   \n",
      "17  US government calls for insurance companies to...   2024-03-10   \n",
      "3   ‘We’re hemorrhaging money’: US health clinics ...   2024-03-09   \n",
      "8   Top US cybersecurity agency hacked and forced ...   2024-03-08   \n",
      "26  Cyberattack on insurance giant disrupting busi...   2024-03-01   \n",
      "1   FTC fines cybersecurity company Avast $16.5 mi...   2024-02-22   \n",
      "\n",
      "                                          Description  \\\n",
      "37  A cyberattack has disrupted “clinical operatio...   \n",
      "17  The US government on Sunday urged insurance co...   \n",
      "3   For more than two weeks, a cyberattack has dis...   \n",
      "8   A federal agency in charge of cybersecurity di...   \n",
      "26  A week after a cyberattack disrupted insurance...   \n",
      "1   Cybersecurity software company Avast faces a $...   \n",
      "\n",
      "                                                  Url Author  \n",
      "37  https://www.cnn.com/2024/05/08/tech/cyberattac...    CNN  \n",
      "17  https://www.cnn.com/2024/03/10/tech/govt-lette...    CNN  \n",
      "3   https://www.cnn.com/2024/03/09/tech/medical-su...    CNN  \n",
      "8   https://www.cnn.com/2024/03/08/politics/top-us...    CNN  \n",
      "26  https://www.cnn.com/2024/02/28/tech/cyberattac...    CNN  \n",
      "1   https://www.cnn.com/2024/02/22/tech/ftc-avast-...    CNN  \n"
     ]
    }
   ],
   "source": [
    " \n",
    "print(news_df_filtered)\n",
    "news_df_filtered.to_csv('OutputTestFiles/cnn_test.csv',sep=\";\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
