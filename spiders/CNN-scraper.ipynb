{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from scrapy.selector import Selector\n",
    "import time\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    " \n",
    "\n",
    "search_str = 'cybersecurity financial industry'\n",
    "last_x_weeeks = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Search terms defined abvove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allowed_domains = ['www.cnn.com']\n",
    "start_urls = ['https://www.cnn.com']\n",
    "\n",
    "# set up the driver\n",
    "chrome_options = Options()\n",
    "# chrome_options.add_argument(\"--headless\") # uncomment if don't want to appreciate the sight of a possessed browser\n",
    "driver = webdriver.Chrome(chrome_options)\n",
    "driver.get(\"https://edition.cnn.com/search\")\n",
    "\n",
    "\n",
    "\n",
    "search_input = driver.find_element(by = By.XPATH,value=\"//*[@id='search']/div[1]/div[1]/input\") # find the search bar\n",
    "search_input.send_keys(search_str) # type in the search term\n",
    "\n",
    "time.sleep(1)\n",
    "search_btn = driver.find_element(by = By.XPATH,value = \"//*[@id='search']/div[1]/div[1]/button[2]\") # find the search button\n",
    "search_btn.click()\n",
    "\n",
    "time.sleep(1)\n",
    "stories_label = driver.find_element(by = By.XPATH,value = \"//*[@id='search']/div[1]/div[2]/div/div/ul/li[2]/label\") # find the search button\n",
    "stories_label.click()\n",
    "\n",
    "time.sleep(1)\n",
    "search_btn = driver.find_element(by = By.XPATH,value = \"//*[@id='relevance']\") # find the search button\n",
    "search_btn.click()\n",
    "\n",
    "time.sleep(2)\n",
    "driver.refresh()\n",
    "time.sleep(2)\n",
    "\n",
    "html = [driver.page_source]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Define function to scrap each news page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pages(html):\n",
    "    rows = []\n",
    "    for page in html:\n",
    "        resp = Selector(text = page)\n",
    "        #print(resp)\n",
    "        results = resp.css(\"#search > div.search__right > div > div.search__results-list > div > div.container_list-images-with-description__cards-wrapper > div > div\") # result iterator\n",
    "        #print(results)    \n",
    "        dataUriList =results.xpath(\"//*[@id='search']/div[2]/div/div[2]/div/div[2]/div/div\")\n",
    "        news_url = dataUriList.xpath(\"//*[@class='container__headline-text']\")\n",
    "        #print(news_url)\n",
    "        row = []\n",
    "        for i,item in enumerate(news_url):\n",
    "            \n",
    "            \n",
    "            title = item.xpath(\".//text()\").get()\n",
    "            #print(title)\n",
    "\n",
    "            \n",
    "            date_div = item.xpath(f\"//*[@id='search']/div[2]/div/div[2]/div/div[2]/div/div/div[{i+1}]/a[2]/div/div[2]\")\n",
    "            date = date_div.xpath(\".//text()\").get().strip()\n",
    "            datetime_parsed =datetime.strptime(date, \"%b %d, %Y\")\n",
    "            #print(date)\n",
    "\n",
    "\n",
    "            link = item.xpath(\".//@data-zjs-href\").get()\n",
    "            #print(link)\n",
    "\n",
    "\n",
    "            description_div = item.xpath(f\"//*[@id='search']/div[2]/div/div[2]/div/div[2]/div/div/div[{i+1}]/a[2]/div/div[3]\")\n",
    "            description = description_div.xpath(\".//text()\").get().strip()\n",
    "            #print(description)\n",
    "            author = 'CNN'\n",
    "            row = [title,datetime_parsed,description,link,author]\n",
    "            rows.append(row)\n",
    "\n",
    "        return rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Poll each page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = ['Title','Publish Date','Description','Url','Author']\n",
    "news = get_pages(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    \n",
    "    i+=1\n",
    "    time.sleep(2)\n",
    "    next_btn = driver.find_element(by = By.XPATH,value = \"//*[@id='search']/div[2]/div/div[4]/div/div[3]\")\n",
    "    next_btn.click()\n",
    "    time.sleep(2)\n",
    "    driver.refresh()\n",
    "    time.sleep(2)\n",
    "    news.extend(get_pages([driver.page_source]))\n",
    "\n",
    "print(len(news))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Order and filter news by date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_date = datetime.today()-timedelta(weeks=last_x_weeeks)\n",
    "\n",
    "\n",
    "news_df = pd.DataFrame(data=news,columns=headers)\n",
    "news_df_sorted = news_df.sort_values(by= 'Publish Date', ascending=False)\n",
    "news_df_filtered = news_df_sorted[news_df_sorted['Publish Date']>=filter_date]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Build the output file to be used as chat gpt input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Title Publish Date  \\\n",
      "12  The auto dealers outage has been hamstringing ...   2024-06-27   \n",
      "17  Microsoft president to Congress: ‘We accept re...   2024-06-13   \n",
      "10  Cyberattack disrupts operations at major US he...   2024-05-08   \n",
      "34  Police take down $249-a-month global phishing ...   2024-04-18   \n",
      "\n",
      "                                          Description  \\\n",
      "12  Cyberattacks seem to be more devastating than ...   \n",
      "17  Microsoft “accepts responsibility for each and...   \n",
      "10  A cyberattack has disrupted “clinical operatio...   \n",
      "34  Law enforcement officials in 19 countries have...   \n",
      "\n",
      "                                                  Url Author  \n",
      "12  https://www.cnn.com/2024/06/27/business/cdk-gl...    CNN  \n",
      "17  https://www.cnn.com/2024/06/13/tech/microsoft-...    CNN  \n",
      "10  https://www.cnn.com/2024/05/08/tech/cyberattac...    CNN  \n",
      "34  https://www.cnn.com/2024/04/18/tech/labhost-cy...    CNN  \n"
     ]
    }
   ],
   "source": [
    " \n",
    "print(news_df_filtered)\n",
    "news_df_filtered.to_csv('OutputTestFiles/cnn_test.csv',sep=\";\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
